"""Gemini LLM service with career counselor persona."""

import google.generativeai as genai
from typing import Tuple, Optional

from src.config.config import config
from src.utils.logger import logger


# System prompt for career counselor persona
SYSTEM_PROMPT = """
Ð¢Ñ‹ â€” Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ„Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¾Ð»Ð¾Ð³ ÐšÐ°Ð·Ð°Ñ…ÑÑ‚Ð°Ð½Ð° Ñ 15-Ð»ÐµÑ‚Ð½Ð¸Ð¼ Ð¾Ð¿Ñ‹Ñ‚Ð¾Ð¼ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð² ÑÑ„ÐµÑ€Ðµ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ.

Ð¢Ð²Ð¾Ð¹ ÑÑ‚Ð¸Ð»ÑŒ Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ:
- Ð­Ð¼Ð¿Ð°Ñ‚Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¸ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‰Ð¸Ð¹
- Ð—Ð°Ð´Ð°Ñ‘ÑˆÑŒ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÑŽÑ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
- Ð”Ð°Ñ‘ÑˆÑŒ Ñ€ÐµÐ°Ð»Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ðµ, Ð½Ð¾ Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ðµ ÑÐ¾Ð²ÐµÑ‚Ñ‹
- Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑˆÑŒ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²ÑƒÑŽ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑŽ ÑÐµÐ¼ÑŒÐ¸
- Ð“Ð¾Ð²Ð¾Ñ€Ð¸ÑˆÑŒ Ð½Ð° Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾Ð¼ ÑÐ·Ñ‹ÐºÐµ, Ð¸Ð·Ð±ÐµÐ³Ð°Ñ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð¾Ð²

Ð’Ð¡Ð•Ð“Ð”Ð ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€ÑƒÐ¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ‚Ð°Ðº:

1. ðŸ“Š **ÐÐ½Ð°Ð»Ð¸Ð· ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸ ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚Ð°**
   ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹

2. ðŸŽ¯ **Ð¢Ð¾Ð¿-3 Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ñ Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼:**
   Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚Ð° ÑƒÐºÐ°Ð¶Ð¸:
   - ðŸ›ï¸ Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚ + ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ
   - âœ… ÐŸÐ¾Ñ‡ÐµÐ¼Ñƒ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚
   - ðŸ“‹ Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ ÑˆÐ°Ð½ÑÑ‹ Ð¿Ð¾ÑÑ‚ÑƒÐ¿Ð»ÐµÐ½Ð¸Ñ (Ð±Ð°Ð»Ð»Ñ‹ Ð•ÐÐ¢, Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ñ‹)
   - ðŸ’° Ð¡Ñ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (ÐµÑÐ»Ð¸ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾)

3. ðŸ”„ **ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹**
   Ð£ÐºÐ°Ð¶Ð¸ 1-2 Ð·Ð°Ð¿Ð°ÑÐ½Ñ‹Ñ… Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð°

4. ðŸ“ **ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹**
   ÐŸÐ¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾ÑÑ‚ÑƒÐ¿Ð»ÐµÐ½Ð¸Ñ

5. ðŸ’ª **ÐœÐ¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ Ð·Ð°ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ**
   ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸ Ð°Ð±Ð¸Ñ‚ÑƒÑ€Ð¸ÐµÐ½Ñ‚Ð°

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÐ¼Ð¾Ð´Ð·Ð¸ Ð´Ð»Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹, Ð¿Ð¸ÑˆÐ¸ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ, Ð±ÑƒÐ´ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼ Ñ Ñ†Ð¸Ñ„Ñ€Ð°Ð¼Ð¸.
Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð»Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°, Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼ Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ ÑƒÑ‚Ð¾Ñ‡Ð½Ð¸Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.
"""


class LLMService:
    """Gemini LLM service for generating responses."""
    
    def __init__(self, api_key: str):
        if not api_key or api_key == "your_gemini_api_key_here":
            raise ValueError("Valid Gemini API key is required")
        
        genai.configure(api_key=api_key)
        
        # Configure model
        self.model = genai.GenerativeModel(
            model_name="gemini-1.5-flash",
            generation_config={
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": 2048,
            },
            safety_settings=[
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]
        )
        
        logger.info("Gemini LLM service initialized")
    
    def generate_answer(
        self, 
        question: str, 
        context: str, 
        temperature: float = 0.7
    ) -> Tuple[str, Optional[int]]:
        """Generate answer based on context."""
        try:
            # Build prompt
            prompt = f"""{SYSTEM_PROMPT}

---

ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢ (Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾Ð± ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚Ð°Ñ… ÐšÐ°Ð·Ð°Ñ…ÑÑ‚Ð°Ð½Ð°):
{context}

---

Ð’ÐžÐŸÐ ÐžÐ¡ ÐÐ‘Ð˜Ð¢Ð£Ð Ð˜Ð•ÐÐ¢Ð:
{question}

---

Ð”Ð°Ð¹ Ñ€Ð°Ð·Ð²ÐµÑ€Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¢ÐžÐ›Ð¬ÐšÐž Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð²Ñ‹ÑˆÐµ.
Ð•ÑÐ»Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ð½ÐµÑ‚ Ð½ÑƒÐ¶Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼.
"""
            
            # Generate response
            response = self.model.generate_content(prompt)
            
            answer = response.text
            tokens_used = None
            
            # Try to get token count
            try:
                if hasattr(response, 'usage_metadata'):
                    tokens_used = response.usage_metadata.total_token_count
            except:
                pass
            
            logger.info(f"Generated response ({len(answer)} chars)")
            return answer, tokens_used
            
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            return f"Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {str(e)}", None
    
    def check_health(self) -> str:
        """Check if Gemini API is accessible."""
        try:
            response = self.model.generate_content("ÐŸÑ€Ð¸Ð²ÐµÑ‚")
            return "connected"
        except Exception as e:
            logger.error(f"Gemini health check failed: {e}")
            return f"error: {str(e)}"
